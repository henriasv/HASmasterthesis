\chapter{Details of using the Abel computing cluster}

\section{Compiling lammps on the Abel computing cluster}
It is usually quite straightforward to install the main features of LAMMPS. However, if one is to use special features such as GPU packages or th Intel Xeon Phi, it is more complicated... 	

To compile LAMMPS on the Abel computer cluster, one has to load the intel compiler and mpi modules, and then follow the build instructions from the LAMMPS documentation. The program used for simulations going into this thesis, is compiled with the following command:
\begin{lstlisting}[language=Bash]
# Script to build lammps with openMP and Intel on Abel (November 21. 2014)
module load intel
module load intelmpi.intel
make yes-user-intel
make yes-user-omp
make yes-kspace
make yes-replica
make yes-molecule
make yes-rigid
make intel_cpu
\end{lstlisting}
These commands are run from {\tt src} in the LAMMPS install folder, which was extracted from a tarball.

\section{Submitting jobs}
When expanding templates, a corresponding job script is also generated. It typically looks like:
\begin{lstlisting}[language=Bash]
#!/bin/bash
# Job name:
#SBATCH --job-name=s1_hydrate_crack.in2015-02-12T11:17:08.271798
# Project: 
#SBATCH --account=nn9272k
# Wall clock limit:
#SBATCH --time='10:00:00'
#SBATCH --mem-per-cpu=4000M
# CPUs:
#SBATCH --nodes=18 --ntasks-per-node=1 --cpus-per-task=15
module purge
module load intel
module load intelmpi.intel
mpirun -np 18 lmp_intel_cpu -package omp 15 -suffix omp -in s1_hydrate_crack.in
\end{lstlisting}

This job script starts LAMMPS in a hybrid MPI and OpenMP mode with 18 mpi processes, each with 15 cpu's. Note the {\tt -suffix omp} which tells LAMMPS to use the OpenMP version of any potential and function in LAMMPS. If features that do not support OpenMP are used, then the simulation might run slowly, and resources might remain unused (idle cpu's).

Note: since it turned out that I could not use the OpenMP package, the scripts I actually used does not contain the OpenMP commands, and {\tt --cpus-per-task} is set to 1.

\section{Experienced problems when using the Abel computing cluster}
When performing large-scale simulations, failures on the computing cluster can consume a lot of time.
\begin{itemize}
\item Random node failures
\item Slow nodes -- killed jobs
\end{itemize}


I have made a small python module that lets me submit a job for each subdirectory containing a file {\tt lmp\_slurm\_job.sh}:

\begin{lstlisting}[language=Python]
# File: sbatch_tree.py
import subprocess
import argparse
import os.path as op
import os

def walkfunc(arg, dirname, names):
	job_script = op.join(dirname, 'lmp_slurm_job.sh')
	if op.isfile(job_script):
		if op.isfile(op.join(dirname,'log.lammps')):
			print "Seems like simulaton is running or has been run since there are output files in the folder"
			print dirname
		else:
			os.chdir(dirname)
			subprocess.call(['sbatch', 'lmp_slurm_job.sh'])
			print "Submitted from", job_script 

if __name__=='__main__':
	parser = argparse.ArgumentParser()
	parser.add_argument('sim_root_folder', type=str)
	args = parser.parse_args()
	op.walk(args.sim_root_folder, walkfunc, None)
\end{lstlisting}

Usage:
\begin{lstlisting}[language=Bash]
python -m sbatch_tree $PWD
\end{lstlisting}